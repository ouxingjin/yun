2019-03-06

############################################################################
ipvsadm -A|-E|-D   -t/-u  VIP:port  -s wrr
ipvsadm -a/-e/-d  -t/-u   VIP:port -r RIP -g/-m/-i

LVS/NAT :客户端与服务器不在同一个网段
LVS/DR  :客户端与服务器在同一个网段
############################################################################
keepalived就是给LVS写的：
           1.自动配置LVS规则，做健康检查
	   2.学习了路由器上的功能VRRP（与动态路由相似HSRP）--vip浮动ip（取决于优先级）

1 案例1：Keepalived高可用服务器  -----主备结构
准备三台Linux服务器，两台做Web服务器，并部署Keepalived高可用软件，一台作为客户端主机，实现如下功能：
	使用Keepalived实现web服务器的高可用
	Web服务器IP地址分别为192.168.4.100和192.168.4.200 (CIP 192.168.4.5)
	Web服务器的浮动VIP地址为192.168.4.80--------(由keepalived自动配置)
	客户端通过访问VIP地址访问Web页面

步骤一：配置网络环境（如果在前面课程已经完成该配置，可以忽略此步骤）
1）设置Web1服务器网络参数、配置Web服务

2）设置Web2服务器网络参数、配置Web服务  #确保网页服务能开启

步骤二：安装Keepalived软件
        #系统自带，启动服务时有bug，---官网上下载，源码编译安装
[root@web1 ~]# yum install -y keepalived
[root@web2 ~]# yum install -y keepalived  #每次启动keepalived，防火墙会拒绝

步骤三：部署Keepalived服务
1）修改web1服务器Keepalived配置文件
[root@web1 ~]# vim /etc/keepalived/keepalived.conf
global_defs {
  notification_email {
    admin@tarena.com.cn               #设置报警收件人邮箱
  }
  notification_email_from ka@localhost  #设置发件人
  smtp_server 127.0.0.1                 #定义邮件服务器
  smtp_connect_timeout 30
  router_id  web1                       #设置路由ID号（实验需要修改）
}
vrrp_instance VI_1 {
  state MASTER  影响的是初始状态   #主服务器为MASTER（备服务器需要修改为BACKUP）
  interface eth0               #定义网络接口，浮动ip配置在eth0上
  virtual_router_id 50         #主备服务器VRID号必须一致（同一个集群）
  priority 100                 #服务器优先级,优先级高优先获取VIP（实验需要修改）
  advert_int 1                 #间隔时间（间隔1s种比较一次优先级）
  authentication {
    auth_type pass             #设置密码
    auth_pass 1111             #主备服务器密码必须一致（同一个集群）--实际中要自己设置
  }
  virtual_ipaddress { 192.168.4.80    #谁是主服务器谁获得该VIP（实验需要修改）
   }
}
#往后的是自动配置LVS的

2）修改web2服务器Keepalived配置文件:
   router_id web2    #设置路由ID号
   priority 50       #作为备用服务器，优先级小于主服务器
   virtual_ipaddress {192.168.4.80}  #谁是主服务器，就给谁配置VIP

3）启动服务
[root@web1 ~]# systemctl start keepalived
[root@web2 ~]# systemctl start keepalived

4）配置防火墙和SELinux
启动keepalived会自动添加一个drop的防火墙规则，需要清空！
[root@web1 ~]# iptables -F    ---每次启动服务都需要将防火墙规则清空
[root@web1 ~]# setenforce 0
[root@web2 ~]# iptables -F
[root@web1 ~]# setenforce 0

步骤四：测试
1）登录两台Web服务器查看VIP信息
[root@web1 ~]# ip addr show == ip a s 
[root@web2 ~]# ip addr show

日志文件/var/log/messages

2) 客户端访问
客户端使用curl命令连接http://192.168.4.80，查看Web页面；关闭Web1服务器的网卡，客户端再次访问http://192.168.4.80，验证是否可以正常访问服务
#############################################################################

2 案例3：Keepalived+LVS服务器
环境准备：proxy1(LVS1)：eth0 192.168.4.5
                      VIP  192.168.4.15 (keepalived自动配置)
        proxy2(LVS2) :eth0 192.168.4.6
                      VIP  192.168.4.15
 	  web1 ： eth0 192.168.4.100
                VIP 192.168.4.15 (需要自己配置) lo:0
   	  web2 : eth0 192.168.4.200
                VIP 192.168.4.15 (需要自己配置) lo:0
步骤三：调度器安装Keepalived与ipvsadm软件
[root@proxy1 ~]# yum install -y keepalived
[root@proxy1 ~]# systemctl enable keepalived
[root@proxy1 ~]# yum install -y ipvsadm
[root@proxy1 ~]# ipvsadm -C

[root@proxy2 ~]# yum install -y keepalived
[root@proxy2 ~]# systemctl enable keepalived
[root@proxy2 ~]# yum install -y ipvsadm
[root@proxy2 ~]# ipvsadm -C


步骤四：部署Keepalived实现LVS-DR模式调度器的高可用
1）LVS1调度器设置Keepalived，并启动服务
[root@proxy1 ~]# vim /etc/keepalived/keepalived.conf
global_defs {
  notification_email {
    admin@tarena.com.cn                  #设置报警收件人邮箱
  }
  notification_email_from ka@localhost   #设置发件人
  smtp_server 127.0.0.1                  #定义邮件服务器
  smtp_connect_timeout 30
  router_id  lvs1                        #设置路由ID号(实验需要修改)
}
vrrp_instance VI_1 {
  state MASTER                            #主服务器为MASTER
  interface eth0                          #定义网络接口
  virtual_router_id 50                    #主辅VRID号必须一致
  priority 100                            #服务器优先级
  advert_int 1
  authentication {
    auth_type pass
    auth_pass 1111                         #主辅服务器密码必须一致
  }
  virtual_ipaddress {  192.168.4.15  }     #配置VIP（实验需要修改）
}
virtual_server 192.168.4.15 80 {           #设置ipvsadm的VIP规则（实验需要修改）
  delay_loop 6
  lb_algo wrr                              #设置LVS调度算法为WRR
  lb_kind DR                               #设置LVS的模式为DR
  #persistence_timeout 50
#注意这样的作用是保持连接，开启后，客户端在一定时间内始终访问相同服务器
  protocol TCP
  real_server 192.168.4.100 80 {           #设置后端web服务器真实IP（实验需要修改）
    weight 1                               #设置权重为1
    TCP_CHECK { 端口   HHTP_GET{检查网页} SSL_GET{}  #对后台real_server做健康检查
    connect_timeout 3
    nb_get_retry 3
    delay_before_retry 3
    }
  }
 real_server 192.168.4.200 80 {       #设置后端web服务器真实IP（实验需要修改）
    weight 2                          #设置权重为2
    TCP_CHECK {
    connect_timeout 3       #超时时间
    nb_get_retry 3          #尝试3次
    delay_before_retry 3    #每隔多久做一次健康检查
    }
  }
}
[root@proxy1 ~]# systemctl start keepalived
[root@proxy1 ~]# ipvsadm -Ln                     #查看LVS规则
[root@proxy1 ~]# ip a  s                          #查看VIP配置

************************************************************************************8
real server {
     TCP_CHECK{端口}
      HTTP_GET{
           url{
       	  path= /a.jsp
		  digest shemzhi
        }
      }
       SSL_GET{}
  }
*************************************************************************************
2）LVS2调度器设置Keepalived
[root@proxy ~]# scp /etc/keepalived/keepalived.conf 192.168.4.6:/etc/keepalived/

[root@proxz ~]# vim /etc/keepalived/keepalived.conf
     router_id web2
     priority 50      #备份调度器优先级小于主服务器

[root@proxy2 ~]# systemctl start keepalived
[root@proxy2 ~]# iptables -F  #清空防火墙策略（每次重启都要执行一次）
[root@proxy2 ~]# ipvsadm -Ln                 #查看LVS规则
[root@proxy2 ~]# ip  a   s                    #查看VIP设置

步骤五：客户端测试
[root@client ~]# curl http://192.168.4.15   #没有单点故障

主备调度器可随便坏一台，real server 可随便坏一台（只有2台的情况）
#############################################################################
3 案例1：配置HAProxy负载平衡集群
速度：   nginx <  LVS <  haproxy  < F5 big-ip（硬件）
LVS 4层调度，不支持7层
HAproxy对正则的支持不如nginx
HAproxy 4，7层调度器
nginx 4，7层调度器

准备4台Linux服务器，两台做Web服务器，1台安装HAProxy，1台做客户端，实现如下功能：
	客户端访问HAProxy，HAProxy分发请求到后端Real Server
	开启HAProxy监控页面，及时查看调度器状态
	设置HAProxy为开机启动

3.3 步骤-------------------------------------------------------------环境：最原始的环境：nginx代理的时候
实现此案例需要按照如下步骤进行。
注意事项：
	将前面实验VIP、LVS等实验的内容清理干净！！！！！！
	删除所有设备的VIP，清空所有LVS设置，关闭keepalived！！！
web1关闭多余的网卡与VIP，配置本地真实IP地址。
[root@web1 ~]# ifdown eth0
[root@web1 ~]# ifdown lo:0

proxy关闭keepalived服务，清理LVS规则。
[root@proxy ~]# systemctl stop keepalived
[root@proxy ~]# systemctl disable keepalived
[root@proxy ~]# ipvsadm -C

步骤一：配置后端Web服务器
设置两台后端Web服务（如果已经配置完成，可用忽略此步骤）


步骤二：部署HAProxy服务器
1）配置网络，安装软件
[root@haproxy ~]# echo 'net.ipv4.ip_forward = 1' >> sysctl.conf  #开启路由转发
[root@haproxy ~]# sysctl -p
[root@haproxy ~]# yum -y install haproxy

2）修改配置文件
[root@haproxy ~]# vim /etc/haproxy/haproxy.cfg
***********************************************************************************
global 全局设置
    maxconn=20000
defaults 默认模式
     maxconn=1000
listen  集群1
       maxconn=500
listen 集群2
        默认值
***********************************************************************************
[root@haproxy ~]# vim /etc/haproxy/haproxy.cfg
global
 log 127.0.0.1 local2   ###[err warning info debug]
 chroot /usr/local/haproxy
 pidfile /var/run/haproxy.pid ###haproxy的pid存放路径
 maxconn 4000     ###最大连接数，默认4000
 user haproxy
 group haproxy
 daemon       ###创建1个进程进入deamon模式运行
defaults
 mode http    ###默认的模式mode { tcp|http|health } log global   ###采用全局定义的日志
 option dontlognull      ###不记录健康检查的日志信息
 option httpclose        ###每次请求完毕后主动关闭http通道
 option httplog          ###日志类别http日志格式
 option forwardfor       ###后端服务器可以从Http Header中获得客户端ip
 option redispatch       ###serverid服务器挂掉后强制定向到其他健康服务器
 timeout connect 10000   #如果backend没有指定，默认为10s
 timeout client 300000   ###客户端连接超时
 timeout server 300000   ###服务器连接超时
 maxconn  60000          ###最大连接数
 retries  3              ###3次连接失败就认为服务不可用，也可以通过后面设置
listen stats
    bind 0.0.0.0:1080    #监听端口
    stats refresh 30s    #统计页面自动刷新时间
    stats uri /stats     #统计页面url
    stats realm Haproxy Manager     #统计页面密码框上提示文本
    stats auth admin:good           #统计页面用户名和密码设置
  #stats hide-version               #隐藏统计页面上HAProxy的版本信息
listen  websrv-rewrite 0.0.0.0:80   #定义集群
   balance roundrobin               #采用轮询算法
   server  web1 192.168.2.100:80 check inter 2000 rise 2 fall 5
   server  web2 192.168.2.200:80 check inter 2000 rise 2 fall 5

*****************************************************************************************
定义集群的两种方法：
1）frontend+backend  前端+后端
   frontend  名字（abc） *:端口
        use_backend   名字“static”
   backend static
      balance     roundrobin
      server      static 127.0.0.1:4331 check
2）listen  名字  *：80
      balance roundrobin  #采用轮询的算法
      server  web1   192.168.2.100：80  #真实机1
      server  web2   192.168.2.200：80  #真实机2
****************************************************************************************
3）启动服务器并设置开机启动
[root@haproxy ~]# systemctl start haproxy
[root@haproxy ~]# systemctl enable haproxy

文档/手册：/usr/share/doc/haproxy-1.5.18/configuration.txt


keepalived + LVS
nginx+keepalived



















